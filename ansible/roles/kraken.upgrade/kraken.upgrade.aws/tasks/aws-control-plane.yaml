- set_fact:
    cluster: "{{ cluster_with_nodepool[0] }}"

- set_fact:
    nodepool: "{{ cluster_with_nodepool[1]}}"

- name: Look up and set k8s minor version for this cluster
  set_fact:
    kubernetes_minor_version: "{{ kubernetes_minor_versions[cluster.name] }}"

- name: Execute appropriate kubectl per minor version
  set_fact:
    kubectl: "/opt/cnct/kubernetes/{{ kubernetes_minor_version }}/bin/kubectl"

- set_fact:
    aws_region: "{{ cluster.providerConfig.region }}"
    kubeconfig: "{{ config_base | expanduser }}/{{ cluster.name }}/admin.kubeconfig"

- name: Set Expected Node Count
  set_fact:
    expected_master_node_count: "{{ nodepool.count }}"

- name: Collect nodes
  command: >
   {{ kubectl }} --kubeconfig={{ kubeconfig }} get nodes -l nodepool={{ nodepool.name }} -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}'
  register: node_names

- debug:
    msg: "{{ node_names }}"

# TODO: figure out how to only gather these facts once. we only need them once. but per cluster. Possibly: put another level of include_task between main and this one.

- name: Gather EC2 facts
  ec2_remote_facts:
    region: "{{ cluster.providerConfig.region }}"
    aws_access_key: "{{ cluster.providerConfig.authentication.accessKey or omit }}"
    aws_secret_key: "{{ cluster.providerConfig.authentication.accessSecret or omit }}"
    profile: "{{ cluster.providerConfig.authentication.credentialsProfile or omit }}"
  register: instance_info
  ignore_errors: yes

- name: Reset node names dict to empty
  set_fact:
    node_names_and_instance_ids: "{{{}}}"
  when: node_names_and_instance_ids is defined

- name: Create dict with node names and instance ids
  set_fact:
    node_names_and_instance_ids: "{{ node_names_and_instance_ids|default({}) | combine( {item.0: item.1} )  }}"
  with_together:
    - "{{ instance_info.instances|map(attribute='private_dns_name')|list }}"
    - "{{ instance_info.instances|map(attribute='id')|list }}"
  when: item.0 in node_names.stdout_lines

- debug:
    msg: "{{ node_names_and_instance_ids }}"
  when: node_names_and_instance_ids is defined #they won't define for etcd and etcdEvent nodes, because they don't write to the command line  with a simple "get no"

# - name: Delete and Terminate Nodes
#   set_fact:
#     node_destruction: "{{ item.key |delete_and_terminate_node_filter(item.value, expected_master_node_count, kubeconfig, aws_region, nodepool.name) }}"
#   with_dict: "{{ node_names_and_instance_ids }}"
#   failed_when: node_destruction == False
