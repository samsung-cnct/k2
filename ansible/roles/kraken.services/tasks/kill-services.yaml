---
  
- set_fact:
    cluster: "{{ cluster_node_tuple.0 }}"
    node: "{{ cluster_node_tuple.1 }}"
- set_fact:
    cluster_services: "{{ cluster.helmConfig.charts | default([]) }}"
    cluster_repos: "{{ cluster.helmConfig.repos | default([]) }}"
    kubeconfig: "{{ config_base | expanduser }}/{{ cluster.name }}/admin.kubeconfig"
    helm_home: "{{ config_base | expanduser }}/{{ cluster.name }}/.helm"
    cluster_namespaces: []
- set_fact:
    cluster_namespaces: "{{ cluster_namespaces + [item.namespace | default('kube-system')] }}"
  with_items: "{{ cluster_services }}"
- set_fact:
    cluster_namespaces: "{{ cluster_namespaces | unique | reject('search', 'kube-system') | list }}"

- name: See if tiller rc if present
  shell: >
      kubectl --kubeconfig={{ kubeconfig }} get deployment {{ tiller }} --namespace=kube-system
  register: tiller_present
  when: kubeconfig | is_file
  ignore_errors: yes

- name: Create Helm home
  file: >
    path={{ helm_home }}
    state=directory

- name: Collect all services
  command: >
    kubectl --kubeconfig={{ kubeconfig }} get services --all-namespaces -o json
  register: added_services
  when: kraken_action == 'down' and kubeconfig | is_file
  ignore_errors: yes

- name: Register services fact
  set_fact:
    added_services_map: "{{ added_services.stdout | from_json }}"
  when: kraken_action == 'down'
  ignore_errors: yes

- name: Set services info
  set_fact:
    the_services: "{{ added_services_map.items()[0][1] }}"
  when: kraken_action == 'down'
  ignore_errors: yes

- name: Clean up releases
  command: >
    helm delete --purge {{ item.name }}
  environment:
    KUBECONFIG: "{{ kubeconfig }}"
    HELM_HOME: "{{ helm_home }}"
  with_items: "{{ cluster_services }}"
  ignore_errors: yes
  when: tiller_present | succeeded and not tiller_present | skipped

- name: Clean up tiller if present
  command: >
    kubectl --kubeconfig={{ kubeconfig }} delete deployment {{ tiller }} --namespace=kube-system
  when: tiller_present | succeeded and not tiller_present | skipped

- name: Clean up services
  command: >
    kubectl --kubeconfig={{ kubeconfig }} delete --namespace {{ item.metadata.namespace }} svc {{ item.metadata.name }}
  with_items: "{{ the_services }}"
  when: item.status.loadBalancer.ingress[0].hostname is defined and kraken_action == 'down'
  ignore_errors: yes

- name: Delete all service namespaces
  command: >
    kubectl --kubeconfig={{ kubeconfig }} delete namespace {{ item }}
  with_items: "{{ cluster_namespaces }}"
  when: cluster_namespaces is defined
  ignore_errors: yes

- name: Get vpc id
  shell: "terraform state show -state={{ config_base | expanduser }}/{{ cluster.name }}/terraform.tfstate module.vpc.aws_vpc.vpc  | awk '/^id/{print $3}'"
  register: terraform_state_show
  when: kraken_action == 'down'
  changed_when: false

- name: Set vpc_id fact
  set_fact:
    vpcid: "{{ terraform_state_show.stdout }}"
  when: kraken_action == 'down'

- name: Wait for ELBs to be deleted
  action:
    module: ec2_elb_facts
    region: "{{ cluster.providerConfig.region }}"
    aws_access_key: "{{ cluster.providerConfig.authentication.accessKey or omit }}"
    aws_secret_key: "{{ cluster.providerConfig.authentication.accessSecret or omit }}"
    profile: "{{ cluster.providerConfig.authentication.credentialsProfile or omit }}"
  register: elb_facts
  vars:
    vpc_lookup: "elbs[?vpc_id=='{{ vpcid }}']"
  when: kraken_action == 'down' and cluster.providerConfig.provider == 'aws'
  until: (elb_facts is none) or (elb_facts | json_query(vpc_lookup) is none) or (elb_facts | json_query(vpc_lookup) | length <= 1)
  retries: 120
  delay: 5
