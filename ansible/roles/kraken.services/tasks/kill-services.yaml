---
- set_fact:
    cluster: "{{ a_cluster }}"

- name: Look up and set k8s minor version for this cluster
  set_fact:
    kubernetes_minor_version: "{{ kubernetes_minor_versions[cluster.name] }}"

- name: Execute appropriate helm per minor version
  set_fact:
    helm_command: "/opt/cnct/kubernetes/{{ kubernetes_minor_version }}/bin/helm"
  when: helm_availabilities[cluster.name] == true

- name: In case of helm not being available, always override to latest helm
  set_fact:
    helm_command: "/opt/cnct/kubernetes/latest/bin/helm"
  when: (helm_availabilities[cluster.name] == false)

- name: Execute appropriate kubectl per minor version
  set_fact:
    kubectl: "/opt/cnct/kubernetes/{{ kubernetes_minor_version }}/bin/kubectl"

- set_fact:
    cluster_services: "{{ cluster.helmConfig.charts | default([]) }}"
    cluster_repos: "{{ cluster.helmConfig.repos | default([]) }}"
    kubeconfig: "{{ config_base | expanduser }}/{{ cluster.name }}/admin.kubeconfig"
    helm_home: "{{ config_base | expanduser }}/{{ cluster.name }}/.helm"
    cluster_namespaces: []

- set_fact:
    cluster_namespaces: "{{ cluster_namespaces + [item.namespace | default('kube-system')] }}"
  with_items: "{{ cluster_services }}"

- set_fact:
    cluster_namespaces: "{{ cluster_namespaces | unique | reject('search', 'kube-system') | list }}"

- name: See if tiller rc if present
  shell: >
      {{ kubectl }} --kubeconfig={{ kubeconfig }} get deployment {{ tiller }} --namespace=kube-system
  register: tiller_present
  when: kubeconfig | is_file
  ignore_errors: yes
  failed_when: false

- name: Create Helm home
  file:
    path: "{{ helm_home }}"
    state: directory

- name: Clean up releases
  command: >
    {{ helm_command }} delete --purge {{ item.name }}
  environment:
    KUBECONFIG: "{{ kubeconfig }}"
    HELM_HOME: "{{ helm_home }}"
  with_items: "{{ cluster_services }}"
  ignore_errors: yes
  when: "'Error' not in tiller_present.stderr and not tiller_present | skipped  and (helm_command is defined) "

- name: Clean up tiller if present
  command: >
    {{ kubectl }} --kubeconfig={{ kubeconfig }} delete deployment {{ tiller }} --namespace=kube-system
  when: "'Error' not in tiller_present.stderr and not tiller_present | skipped "
  ignore_errors: yes

- name: Collect all services
  command: >
    {{ kubectl }} --kubeconfig={{ kubeconfig }} get services --all-namespaces -o json
  register: added_services
  when: kraken_action == 'down' and kubeconfig | is_file
  ignore_errors: yes

- name: Register services fact
  set_fact:
    added_services_map: "{{ added_services.stdout | from_json }}"
  when: kraken_action == 'down'
  ignore_errors: yes

- name: Set services info
  set_fact:
    the_services: "{{ added_services_map['items'] }}"
  when: kraken_action == 'down'
  ignore_errors: yes

- name: Set load balanced service data
  set_fact:
    load_balanced_services: "{{ the_services|json_query('[?status.loadBalancer.ingress[0].hostname != null].{namespace: metadata.namespace, name: metadata.name}') }}"
  when: kraken_action == 'down'
  ignore_errors: yes

- name: Clean up services
  command: >
    {{ kubectl }} --kubeconfig={{ kubeconfig }} delete --namespace {{ item.namespace }} svc {{ item.name }}
  with_items: "{{ load_balanced_services }}"
  when: kraken_action == 'down'
  ignore_errors: yes

- name: Delete all service namespaces
  command: >
    kubectl --kubeconfig={{ kubeconfig }} delete namespace {{ item }}
  with_items: "{{ cluster_namespaces }}"
  when: cluster_namespaces is defined
  ignore_errors: yes

- name: Get vpc id
  shell: "terraform state show -state={{ config_base | expanduser }}/{{ cluster.name }}/terraform.tfstate module.vpc.aws_vpc.vpc  | awk '/^id/{print $3}'"
  register: terraform_state_show
  when: kraken_action == 'down'
  changed_when: false

- name: Set vpc_id fact
  set_fact:
    vpcid: "{{ terraform_state_show.stdout }}"
  when: kraken_action == 'down'

# because BOTO2 doesn't support AWS_SHARED_CREDENTIALS_FILE environment variable, do some mangling
- include: aws_config.yaml
  when: cluster.providerConfig.provider == 'aws'

- name: Wait for ELBs to be deleted
  action:
    module: ec2_elb_facts
    region: "{{ cluster.providerConfig.region }}"
    aws_access_key: "{{ cluster.providerConfig.authentication.accessKey or omit }}"
    aws_secret_key: "{{ cluster.providerConfig.authentication.accessSecret or omit }}"
    profile: "{{ cluster.providerConfig.authentication.credentialsProfile or omit }}"
  register: elb_facts
  vars:
    vpc_lookup: "elbs[?vpc_id=='{{ vpcid }}']"
  when: kraken_action == 'down' and cluster.providerConfig.provider == 'aws'
  until: (elb_facts is none) or (elb_facts | json_query(vpc_lookup) is none) or (elb_facts | json_query(vpc_lookup) | length <= 1)
  retries: 120
  delay: 5